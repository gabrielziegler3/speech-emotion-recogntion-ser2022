{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:  \n",
    "[X] Generate features with ECAPA  \n",
    "[] Test VAD      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k9iQiA7fQ8cO"
   },
   "source": [
    "# Baseline2 - Preprocessing (Wav2Vec Features) - PT-BR-SER (Coraa V6)\n",
    "\n",
    "This is an example of pre-processing audio segments to extract features through the pre-trained Wav2Vec model (small). We use the Fairseq toolkit. No fine-tuning was performed and we only used the audio embedding extracted from the pre-trained model.\n",
    "\n",
    "We use raw audio, i.e., no pre-processing or cleaning has been done beforehand. Participants can pre-process the audio to improve the quality of extracted features. We also recommend using the latest pre-trained models, fine-tuning and data augmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HHgw-hpB82nt"
   },
   "source": [
    "# Download Dataset\n",
    "\n",
    "* https://drive.google.com/drive/folders/12Nuv8J7pBHJuNU3nH2c7F8VwCDEE6GDt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "from speechbrain.pretrained import EncoderClassifier\n",
    "import glob\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = glob.glob('../../data/train/*.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = EncoderClassifier.from_hparams(source=\"speechbrain/spkrec-ecapa-voxceleb\")\n",
    "def predict_ecapa(filepath):\n",
    "    signal, fs = torchaudio.load(filepath)\n",
    "    return classifier.encode_batch(signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "UKehFfxx9mUi"
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pandas as pd\n",
    "\n",
    "train_data_dir = 'train'\n",
    "\n",
    "train_audio_files = paths\n",
    "\n",
    "L = []\n",
    "for f in train_audio_files:\n",
    "  v = f.replace('.wav','').split('_')\n",
    "  L.append([f,v[len(v)-1]])\n",
    "\n",
    "df_data = pd.DataFrame(L)\n",
    "df_data.columns = ['sound_filepath','label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [06:33<00:00,  1.59it/s]\n"
     ]
    }
   ],
   "source": [
    "embeddings = [predict_ecapa(i).flatten().numpy() for i in tqdm(paths)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_embeddings = pd.DataFrame(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_embeddings['sound_filepath'] = paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ecapatdnn = pd.merge(df_embeddings,df_data, on ='sound_filepath')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "8vyihV8i-U3c",
    "outputId": "df7d6057-264d-40e7-c369-33e5f238fb09",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_ecapatdnn.label.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t45VreMWBg6W"
   },
   "source": [
    "# ECAPA-TDNN Features\n",
    "\n",
    "Related methods.\n",
    "\n",
    "> Desplanques, Brecht, Jenthe Thienpondt, and Kris Demuynck. \"Ecapa-tdnn: Emphasized channel attention, propagation and aggregation in tdnn based speaker verification.\" arXiv preprint arXiv:2005.07143 (2020).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "BR0X2hiMt35s"
   },
   "outputs": [],
   "source": [
    "df_ecapatdnn.to_csv('ecapatdnn_features.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO apply VAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting speechbrain\n",
      "  Downloading speechbrain-0.5.10-py3-none-any.whl (393 kB)\n",
      "\u001b[K     |████████████████████████████████| 393 kB 2.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: sentencepiece in /home/victor/anaconda3/envs/torchaudio/lib/python3.8/site-packages (from speechbrain) (0.1.95)\n",
      "Requirement already satisfied, skipping upgrade: joblib in /home/victor/anaconda3/envs/torchaudio/lib/python3.8/site-packages (from speechbrain) (0.16.0)\n",
      "Requirement already satisfied, skipping upgrade: scipy in /home/victor/anaconda3/envs/torchaudio/lib/python3.8/site-packages (from speechbrain) (1.5.1)\n",
      "Requirement already satisfied, skipping upgrade: torch in /home/victor/anaconda3/envs/torchaudio/lib/python3.8/site-packages (from speechbrain) (1.8.1)\n",
      "Requirement already satisfied, skipping upgrade: torchaudio in /home/victor/anaconda3/envs/torchaudio/lib/python3.8/site-packages (from speechbrain) (0.8.1)\n",
      "Requirement already satisfied, skipping upgrade: tqdm in /home/victor/anaconda3/envs/torchaudio/lib/python3.8/site-packages (from speechbrain) (4.48.0)\n",
      "Requirement already satisfied, skipping upgrade: packaging in /home/victor/.local/lib/python3.8/site-packages (from speechbrain) (20.4)\n",
      "Requirement already satisfied, skipping upgrade: huggingface-hub in /home/victor/anaconda3/envs/torchaudio/lib/python3.8/site-packages (from speechbrain) (0.0.8)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /home/victor/anaconda3/envs/torchaudio/lib/python3.8/site-packages (from speechbrain) (1.18.5)\n",
      "Requirement already satisfied, skipping upgrade: hyperpyyaml in /home/victor/anaconda3/envs/torchaudio/lib/python3.8/site-packages (from speechbrain) (1.0.0)\n",
      "Requirement already satisfied, skipping upgrade: typing-extensions in /home/victor/anaconda3/envs/torchaudio/lib/python3.8/site-packages (from torch->speechbrain) (3.7.4.3)\n",
      "Requirement already satisfied, skipping upgrade: six in /home/victor/anaconda3/envs/torchaudio/lib/python3.8/site-packages (from packaging->speechbrain) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /home/victor/.local/lib/python3.8/site-packages (from packaging->speechbrain) (2.4.7)\n",
      "Requirement already satisfied, skipping upgrade: requests in /home/victor/anaconda3/envs/torchaudio/lib/python3.8/site-packages (from huggingface-hub->speechbrain) (2.24.0)\n",
      "Requirement already satisfied, skipping upgrade: filelock in /home/victor/anaconda3/envs/torchaudio/lib/python3.8/site-packages (from huggingface-hub->speechbrain) (3.0.12)\n",
      "Requirement already satisfied, skipping upgrade: ruamel.yaml>=0.15 in /home/victor/anaconda3/envs/torchaudio/lib/python3.8/site-packages (from hyperpyyaml->speechbrain) (0.17.4)\n",
      "Requirement already satisfied, skipping upgrade: pyyaml>=5.1 in /home/victor/anaconda3/envs/torchaudio/lib/python3.8/site-packages (from hyperpyyaml->speechbrain) (5.3.1)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /home/victor/anaconda3/envs/torchaudio/lib/python3.8/site-packages (from requests->huggingface-hub->speechbrain) (2.10)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /home/victor/anaconda3/envs/torchaudio/lib/python3.8/site-packages (from requests->huggingface-hub->speechbrain) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /home/victor/anaconda3/envs/torchaudio/lib/python3.8/site-packages (from requests->huggingface-hub->speechbrain) (2020.6.20)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/victor/anaconda3/envs/torchaudio/lib/python3.8/site-packages (from requests->huggingface-hub->speechbrain) (1.25.10)\n",
      "Requirement already satisfied, skipping upgrade: ruamel.yaml.clib>=0.1.2; platform_python_implementation == \"CPython\" and python_version < \"3.10\" in /home/victor/anaconda3/envs/torchaudio/lib/python3.8/site-packages (from ruamel.yaml>=0.15->hyperpyyaml->speechbrain) (0.2.2)\n",
      "Installing collected packages: speechbrain\n",
      "  Attempting uninstall: speechbrain\n",
      "    Found existing installation: speechbrain 0.5.7\n",
      "    Uninstalling speechbrain-0.5.7:\n",
      "      Successfully uninstalled speechbrain-0.5.7\n",
      "Successfully installed speechbrain-0.5.10\n"
     ]
    }
   ],
   "source": [
    "! pip install -U speechbrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.5.10'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import speechbrain\n",
    "speechbrain.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a71b6f4005649089459415960f4f7f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=2297.0, style=ProgressStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "066e9d262a474d2da4c388c071bbe2a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=452671.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e645844affc94d01b27c4e8d1ea37bc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1063.0, style=ProgressStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from speechbrain.pretrained import VAD\n",
    "\n",
    "VAD = VAD.from_hparams(source=\"speechbrain/vad-crdnn-libriparty\", savedir=\"pretrained_models/vad-crdnn-libriparty\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1- Let's compute frame-level posteriors first\n",
    "audio_file = paths[0]\n",
    "prob_chunks = VAD.get_speech_prob_file(audio_file)\n",
    "# 2- Let's apply a threshold on top of the posteriors\n",
    "prob_th = VAD.apply_threshold(prob_chunks).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# 3- Let's now derive the candidate speech segments\n",
    "boundaries = VAD.get_boundaries(prob_th)\n",
    "\n",
    "# 4- Apply energy VAD within each candidate speech segment (optional)\n",
    "\n",
    "boundaries = VAD.energy_VAD(audio_file,boundaries)\n",
    "\n",
    "# 5- Merge segments that are too close\n",
    "boundaries = VAD.merge_close_segments(boundaries, close_th=0.250)\n",
    "\n",
    "# 6- Remove segments that are too short\n",
    "boundaries = VAD.remove_short_segments(boundaries, len_th=0.250)\n",
    "\n",
    "# 7- Double-check speech segments (optional).\n",
    "boundaries = VAD.double_check_speech_segments(boundaries, audio_file,  speech_th=0.5)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Baseline2 - Preprocessing (Wav2Vec Features) - PT-BR-SER (Coraa V6).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit",
   "language": "python",
   "name": "python38364bitb29787b3554c4c00922997ab506a392c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "5f6de19efddf4b1f980c91d353fe5564": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "605961b4b6d741b4b9f21c6873a153c5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e1ec4221cb304313a02bb755b9271c1a",
      "placeholder": "​",
      "style": "IPY_MODEL_ae3d992216c04770b634237f58d0feff",
      "value": " 625/625 [01:03&lt;00:00,  9.64it/s]"
     }
    },
    "664929a161164285ad157764716cc7b9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8b20e8407da84727900ac851fd85f49b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_664929a161164285ad157764716cc7b9",
      "max": 625,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9a2b26e2069b4d3c886dbd6c94699463",
      "value": 625
     }
    },
    "9a2b26e2069b4d3c886dbd6c94699463": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a372892336494f91a64abb81ffea1e37": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ae3d992216c04770b634237f58d0feff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b7cad1b76a5b4cfabc25ebbaa0c1d776": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a372892336494f91a64abb81ffea1e37",
      "placeholder": "​",
      "style": "IPY_MODEL_5f6de19efddf4b1f980c91d353fe5564",
      "value": "100%"
     }
    },
    "bbb18e4b8a2448d29232a0d6b3627730": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b7cad1b76a5b4cfabc25ebbaa0c1d776",
       "IPY_MODEL_8b20e8407da84727900ac851fd85f49b",
       "IPY_MODEL_605961b4b6d741b4b9f21c6873a153c5"
      ],
      "layout": "IPY_MODEL_e4da0d00f86b4144ae8bcda1e357637c"
     }
    },
    "e1ec4221cb304313a02bb755b9271c1a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e4da0d00f86b4144ae8bcda1e357637c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
